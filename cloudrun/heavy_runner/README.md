# Heavy Runner - Cloud Run Job

Cloud Run Job that executes ML scripts with high memory (32GB RAM) for large datasets.

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                     System Orchestrator                          │
│                                                                   │
│  Dataset < 50MB, < 50k rows  │  Dataset >= 50MB or >= 50k rows  │
│              ↓                │                ↓                  │
│         E2B Sandbox           │         Cloud Run Job             │
│         (8GB RAM)             │          (32GB RAM)               │
└─────────────────────────────────────────────────────────────────┘
```

Both sandboxes execute the **same script** generated by ML Engineer.

## Execution Flow

1. System uploads to GCS:
   - `datasets/{run_id}/cleaned_data.csv` - Input data
   - `inputs/{run_id}/ml_script.py` - Generated script
   - `inputs/{run_id}/support/data/*.json` - Config files
   - `inputs/{run_id}.json` - Request payload

2. Cloud Run Job:
   - Downloads data to `/tmp/run/data/cleaned_data.csv`
   - Downloads support files to `/tmp/run/data/`
   - Executes script with `cwd=/tmp/run`
   - Uploads outputs to `outputs/{run_id}/`

3. System downloads from GCS:
   - `outputs/{run_id}/data/metrics.json`
   - `outputs/{run_id}/data/scored_rows.csv`
   - `outputs/{run_id}/data/alignment_check.json`

## Deploy

### Prerequisites
- Google Cloud SDK installed and configured
- Docker installed
- Artifact Registry repository created

### Build and Deploy

```bash
# Set variables
PROJECT_ID="your-project-id"
REGION="us-central1"
REPO="heavy-runner"
IMAGE="heavy-train"
JOB_NAME="heavy-runner"

# Build and push image
cd cloudrun/heavy_runner
gcloud builds submit --tag ${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO}/${IMAGE}:latest

# Create/update job
gcloud run jobs create ${JOB_NAME} \
    --image ${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO}/${IMAGE}:latest \
    --region ${REGION} \
    --memory 32Gi \
    --cpu 8 \
    --max-retries 0 \
    --task-timeout 30m \
    --set-env-vars "PYTHONUNBUFFERED=1"

# Or update existing job
gcloud run jobs update ${JOB_NAME} \
    --image ${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO}/${IMAGE}:latest \
    --region ${REGION}
```

### Environment Variables

Set in your `.env` or system environment:

```bash
HEAVY_RUNNER_ENABLED=true
HEAVY_RUNNER_JOB=heavy-runner
HEAVY_RUNNER_REGION=us-central1
HEAVY_RUNNER_BUCKET=your-bucket-name
HEAVY_RUNNER_PROJECT=your-project-id
# Optional: max seconds for user script execution inside the runner
# (default: 1740 seconds, i.e. 29 minutes)
HEAVY_RUNNER_SCRIPT_TIMEOUT_SECONDS=1740
```

## Request Payload (inputs/{run_id}.json)

```json
{
  "run_id": "abc123",
  "dataset_uri": "gs://bucket/datasets/abc123/cleaned_data.csv",
  "output_uri": "gs://bucket/outputs/abc123/",
  "code_uri": "gs://bucket/inputs/abc123/ml_script.py",
  "data_path": "data/cleaned_data.csv",
  "read": {
    "sep": ",",
    "decimal": ".",
    "encoding": "utf-8"
  },
  "support_files": [
    {"uri": "gs://bucket/inputs/abc123/support/data/ml_plan.json", "path": "data/ml_plan.json"},
    {"uri": "gs://bucket/inputs/abc123/support/data/column_sets.json", "path": "data/column_sets.json"}
  ]
}
```

## Outputs

| File | Description |
|------|-------------|
| `data/metrics.json` | Model metrics and CV scores |
| `data/scored_rows.csv` | Predictions for all rows |
| `data/alignment_check.json` | Validation results |
| `artifacts/execution_log.txt` | Script stdout/stderr |
| `error.json` | Error details (if failed) |
| `status.json` | Execution status |

## Local Testing

```bash
# Build image locally
docker build -t heavy-runner .

# Test with local files
docker run -it \
    -e INPUT_URI=/data/request.json \
    -e OUTPUT_URI=/output/ \
    -v $(pwd)/test:/data \
    -v $(pwd)/output:/output \
    heavy-runner
```

## Troubleshooting

### Script fails but no error.json
- Check Cloud Run logs: `gcloud run jobs executions logs JOB_NAME`
- Check `artifacts/execution_log.txt` in GCS outputs

### Missing outputs
- Verify script generates files in correct paths (`data/metrics.json`, etc.)
- Check that script doesn't change working directory

### Timeout
- Default is 30 minutes, increase with `--task-timeout`
- Consider using `safe_mode=true` to limit CV folds
