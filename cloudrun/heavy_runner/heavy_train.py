"""
Heavy Runner - Cloud Run Job for executing ML scripts.

This module provides a Cloud Run Job that executes Python scripts (generated by
ML Engineer or Data Engineer agents) in a sandboxed environment with more
resources than E2B (32GB RAM vs 8GB).

The execution model is identical to E2B:
1. Download dataset to payload data_path
2. Download support files (manifests, configs)
3. Execute the script from the working directory
4. Collect and upload all generated outputs

Environment Variables:
    INPUT_URI: GCS URI to request.json with execution config
    OUTPUT_URI: GCS URI prefix for outputs
"""
import ast
import importlib.util
import json
import os
import shutil
import subprocess
import sys
import time
import traceback
from typing import Any, Dict, List, Optional, Set, Tuple

import joblib
import numpy as np
import pandas as pd
try:
    from google.cloud import storage
except Exception:
    storage = None  # type: ignore[assignment]
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score


# Protocol marker for orchestrator/runner compatibility.
HEAVY_RUNNER_PROTOCOL_VERSION = "de_mode_v1"
DEFAULT_SCRIPT_TIMEOUT_SECONDS = 1740
MIN_SCRIPT_TIMEOUT_SECONDS = 60
MAX_SCRIPT_TIMEOUT_SECONDS = 7200

_DYNAMIC_DEPENDENCY_MAP: Dict[str, Tuple[str, str]] = {
    "torch": ("torch", "torch"),
    "transformers": ("transformers", "transformers"),
    "tokenizers": ("tokenizers", "tokenizers"),
    "datasets": ("datasets", "datasets"),
    "accelerate": ("accelerate", "accelerate"),
    "sentence_transformers": ("sentence-transformers", "sentence_transformers"),
    "sentencepiece": ("sentencepiece", "sentencepiece"),
    "huggingface_hub": ("huggingface-hub", "huggingface_hub"),
    "shap": ("shap", "shap"),
    "xgboost": ("xgboost", "xgboost"),
    "lightgbm": ("lightgbm", "lightgbm"),
    "catboost": ("catboost", "catboost"),
    "optuna": ("optuna", "optuna"),
    "imblearn": ("imbalanced-learn", "imblearn"),
    "category_encoders": ("category_encoders", "category_encoders"),
    "plotly": ("plotly", "plotly"),
    "rapidfuzz": ("rapidfuzz", "rapidfuzz"),
    "pydantic": ("pydantic", "pydantic"),
    "pandera": ("pandera", "pandera"),
    "networkx": ("networkx", "networkx"),
}

_DYNAMIC_DEP_ALIASES: Dict[str, str] = {
    "sentence-transformers": "sentence_transformers",
    "sentence_transformers": "sentence_transformers",
    "huggingface-hub": "huggingface_hub",
    "huggingface_hub": "huggingface_hub",
    "imbalanced-learn": "imblearn",
    "imblearn": "imblearn",
}


def log(message: str) -> None:
    """Print timestamped log message."""
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] [heavy_runner] {message}", flush=True)


def _ensure_text(value: Optional[Any]) -> str:
    """Coerce bytes/None to safe text for logging."""
    if value is None:
        return ""
    if isinstance(value, bytes):
        return value.decode("utf-8", errors="replace")
    return str(value)


def _normalize_rel_path(path: Any) -> str:
    """Normalize a relative artifact path to portable slash-separated form."""
    if path is None:
        return ""
    normalized = str(path).strip().lstrip("/").replace("\\", "/")
    while "//" in normalized:
        normalized = normalized.replace("//", "/")
    return normalized


def _coerce_timeout_seconds(value: Any) -> Optional[int]:
    try:
        if value is None:
            return None
        parsed = int(str(value).strip())
    except Exception:
        return None
    if parsed <= 0:
        return None
    if parsed < MIN_SCRIPT_TIMEOUT_SECONDS:
        return MIN_SCRIPT_TIMEOUT_SECONDS
    if parsed > MAX_SCRIPT_TIMEOUT_SECONDS:
        return MAX_SCRIPT_TIMEOUT_SECONDS
    return parsed


def _resolve_script_timeout_seconds(payload: Optional[Dict[str, Any]] = None) -> int:
    """
    Resolve execution timeout for user scripts.

    Precedence:
    1) payload.script_timeout_seconds
    2) env HEAVY_RUNNER_SCRIPT_TIMEOUT_SECONDS
    3) DEFAULT_SCRIPT_TIMEOUT_SECONDS
    """
    payload_timeout = None
    if isinstance(payload, dict):
        payload_timeout = _coerce_timeout_seconds(payload.get("script_timeout_seconds"))
    if payload_timeout is not None:
        return payload_timeout

    env_timeout = _coerce_timeout_seconds(os.getenv("HEAVY_RUNNER_SCRIPT_TIMEOUT_SECONDS"))
    if env_timeout is not None:
        return env_timeout

    return DEFAULT_SCRIPT_TIMEOUT_SECONDS


def _extract_import_roots(code_text: str) -> Set[str]:
    text = str(code_text or "")
    try:
        tree = ast.parse(text)
    except Exception:
        return set()
    roots: Set[str] = set()
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                if alias.name:
                    roots.add(alias.name.split(".")[0].strip().lower())
        elif isinstance(node, ast.ImportFrom):
            if node.module:
                roots.add(node.module.split(".")[0].strip().lower())
    return {root for root in roots if root}


def _normalize_dependency_root(value: Any) -> str:
    raw = str(value or "").strip().lower()
    if not raw:
        return ""
    raw = raw.replace("-", "_")
    raw = raw.split(".", 1)[0]
    return _DYNAMIC_DEP_ALIASES.get(raw, raw)


def _module_available(module_name: str) -> bool:
    try:
        return importlib.util.find_spec(module_name) is not None
    except Exception:
        return False


def _dedupe_preserve_order(items: List[str]) -> List[str]:
    out: List[str] = []
    seen: Set[str] = set()
    for item in items:
        name = str(item or "").strip()
        if not name or name in seen:
            continue
        seen.add(name)
        out.append(name)
    return out


def _resolve_dynamic_dependency_plan(payload: Dict[str, Any], script_path: str) -> Dict[str, Any]:
    required_raw = payload.get("required_dependencies")
    required_roots: Set[str] = set()
    if isinstance(required_raw, list):
        required_roots = {
            root
            for root in (_normalize_dependency_root(dep) for dep in required_raw)
            if root
        }

    script_import_roots: Set[str] = set()
    if script_path and os.path.exists(script_path):
        try:
            with open(script_path, "r", encoding="utf-8") as f:
                script_import_roots = {
                    root
                    for root in (_normalize_dependency_root(dep) for dep in _extract_import_roots(f.read()))
                    if root
                }
        except Exception:
            script_import_roots = set()

    requested_roots = required_roots | script_import_roots
    known_roots = set(_DYNAMIC_DEPENDENCY_MAP.keys())
    install_roots = sorted(root for root in requested_roots if root in known_roots)
    unknown_roots = sorted(root for root in requested_roots if root not in known_roots)

    pip_packages: List[str] = []
    missing_roots: List[str] = []
    for root in install_roots:
        pip_pkg, module_probe = _DYNAMIC_DEPENDENCY_MAP[root]
        if _module_available(module_probe):
            continue
        missing_roots.append(root)
        pip_packages.append(pip_pkg)

    return {
        "required_roots": sorted(required_roots),
        "script_import_roots": sorted(script_import_roots),
        "missing_roots": missing_roots,
        "unknown_roots": unknown_roots,
        "pip_packages": _dedupe_preserve_order(pip_packages),
    }


def _install_dynamic_dependencies(packages: List[str]) -> None:
    packages = _dedupe_preserve_order(packages or [])
    if not packages:
        return
    cmd = [sys.executable, "-m", "pip", "install", "--no-cache-dir", *packages]
    log(f"Installing runtime dependencies: {packages}")
    proc = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        timeout=1800,
        check=False,
    )
    if proc.returncode != 0:
        stdout_tail = _ensure_text(proc.stdout)[-2000:]
        stderr_tail = _ensure_text(proc.stderr)[-4000:]
        raise RuntimeError(
            "runtime_dependency_install_failed: "
            + f"packages={packages}; exit_code={proc.returncode}; "
            + f"stdout_tail={stdout_tail}; stderr_tail={stderr_tail}"
        )


def _parse_gs_uri(uri: str) -> Tuple[str, str]:
    """Parse gs:// URI into bucket and blob path."""
    if not uri.startswith("gs://"):
        raise ValueError(f"Invalid GCS URI: {uri}")
    path = uri[len("gs://"):]
    parts = path.split("/", 1)
    bucket = parts[0]
    blob = parts[1] if len(parts) > 1 else ""
    if not bucket or not blob:
        raise ValueError(f"Invalid GCS URI (missing bucket or object): {uri}")
    return bucket, blob


def _normalize_output_uri(uri: str) -> str:
    """Ensure output URI ends with /"""
    if not uri:
        return uri
    return uri if uri.endswith("/") else uri + "/"


def _gcs_client() -> Any:
    """Get GCS client instance."""
    if storage is None:
        raise RuntimeError("google-cloud-storage is required for gs:// URIs but is not installed")
    return storage.Client()


def _download_gcs_to_path(uri: str, local_path: str) -> None:
    """Download a GCS object to local path."""
    bucket_name, blob_name = _parse_gs_uri(uri)
    client = _gcs_client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    blob.download_to_filename(local_path)


def _download_gcs_text(uri: str) -> str:
    """Download GCS object as text."""
    bucket_name, blob_name = _parse_gs_uri(uri)
    client = _gcs_client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    return blob.download_as_text(encoding="utf-8-sig")


def _upload_path_to_gcs(local_path: str, uri: str) -> None:
    """Upload local file to GCS."""
    bucket_name, blob_name = _parse_gs_uri(uri)
    client = _gcs_client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.upload_from_filename(local_path)


def _write_json_output(obj: Dict[str, Any], output_uri: str, filename: str) -> None:
    """Write JSON object to output location."""
    output_uri = _normalize_output_uri(output_uri)
    tmp_path = os.path.join("/tmp", filename.replace("/", "_"))
    with open(tmp_path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, ensure_ascii=True)
    _write_file_output(tmp_path, output_uri, filename)


def _write_file_output(local_path: str, output_uri: str, filename: str) -> None:
    """Write local file to output location (GCS or local)."""
    output_uri = _normalize_output_uri(output_uri)
    if output_uri.startswith("gs://"):
        _upload_path_to_gcs(local_path, output_uri + filename)
        return
    os.makedirs(output_uri, exist_ok=True)
    dest = os.path.join(output_uri, filename)
    os.makedirs(os.path.dirname(dest), exist_ok=True)
    shutil.copy2(local_path, dest)


def _load_input_json(input_uri: str) -> Dict[str, Any]:
    """Load request JSON from URI."""
    if input_uri.startswith("gs://"):
        text = _download_gcs_text(input_uri)
        return json.loads(text)
    with open(input_uri, "r", encoding="utf-8") as f:
        return json.load(f)


def _download_to_path(uri: str, local_path: str) -> None:
    """Download file from URI (GCS or local) to local path."""
    if uri.startswith("gs://"):
        _download_gcs_to_path(uri, local_path)
        return
    if os.path.exists(uri):
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(uri, "rb") as src, open(local_path, "wb") as dst:
            dst.write(src.read())
        return
    raise ValueError(f"Unsupported URI for download: {uri}")


def _download_support_files(items: Any, base_dir: str) -> List[str]:
    """Download support files to working directory. Returns list of downloaded paths."""
    downloaded = []
    if not items:
        return downloaded
    if not isinstance(items, list):
        raise ValueError("support_files must be a list")
    for item in items:
        if not isinstance(item, dict):
            continue
        uri = item.get("uri")
        rel_path = item.get("path")
        if not uri or not rel_path:
            continue
        dest_path = os.path.join(base_dir, rel_path)
        os.makedirs(os.path.dirname(dest_path), exist_ok=True)
        try:
            _download_to_path(uri, dest_path)
            downloaded.append(rel_path)
            log(f"  Downloaded support file: {rel_path}")
        except Exception as e:
            log(f"  Warning: Failed to download {rel_path}: {e}")
    return downloaded


def _run_script(
    script_path: str,
    work_dir: str,
    timeout_seconds: Optional[int] = None,
) -> Tuple[int, str, str]:
    """
    Execute Python script in working directory.

    Args:
        script_path: Path to the Python script
        work_dir: Working directory for execution (cwd)
        timeout_seconds: Execution timeout in seconds

    Returns:
        Tuple of (exit_code, stdout, stderr)
    """
    effective_timeout = _coerce_timeout_seconds(timeout_seconds) or DEFAULT_SCRIPT_TIMEOUT_SECONDS
    try:
        proc = subprocess.run(
            [sys.executable, script_path],
            cwd=work_dir,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=effective_timeout,
        )
        return proc.returncode, proc.stdout, proc.stderr
    except subprocess.TimeoutExpired as e:
        return -1, e.stdout or "", f"TIMEOUT: Script exceeded {effective_timeout}s limit\n{e.stderr or ''}"


def _collect_output_files(work_dir: str, skip_paths: Optional[set[str]] = None) -> List[str]:
    """
    Collect all output files from working directory.

    Scans these directories for outputs:
    - data/ (metrics, scored_rows, alignment_check, etc.)
    - static/plots/ (generated visualizations)
    - report/ (generated reports)
    - artifacts/ (models, logs, etc.)

    Excludes input files (cleaned_data.csv, cleaned_full.csv).
    """
    roots = [
        "data",
        os.path.join("static", "plots"),
        "report",
        "reports",
        "artifacts",
        "models",
    ]
    skip = {_normalize_rel_path(path) for path in (skip_paths or set()) if path}
    collected = []
    for root in roots:
        base = os.path.join(work_dir, root)
        if not os.path.exists(base):
            continue
        for dirpath, _, filenames in os.walk(base):
            for name in filenames:
                path = os.path.join(dirpath, name)
                rel = os.path.relpath(path, work_dir)
                rel_norm = _normalize_rel_path(rel)
                if rel_norm in skip:
                    continue
                collected.append(rel_norm)
    return sorted(set(collected))


def _collect_existing_required_outputs(
    work_dir: str,
    required: List[str],
    skip_paths: Optional[Set[str]] = None,
) -> List[str]:
    """
    Collect contract-required outputs that exist, even if produced outside scan roots.

    This keeps upload behavior aligned to contract.required_outputs instead of
    relying on static folder heuristics.
    """
    required_existing: List[str] = []
    skip = {_normalize_rel_path(path) for path in (skip_paths or set()) if path}
    work_dir_abs = os.path.abspath(work_dir)
    for rel_path in required:
        rel_norm = _normalize_rel_path(rel_path)
        if not rel_norm or rel_norm in skip:
            continue
        full_path = os.path.abspath(os.path.join(work_dir, rel_norm))
        if not (full_path == work_dir_abs or full_path.startswith(work_dir_abs + os.sep)):
            continue
        if os.path.isfile(full_path):
            required_existing.append(rel_norm)
    return sorted(set(required_existing))


def _check_required_outputs(work_dir: str, required: List[str]) -> Tuple[List[str], List[str]]:
    """
    Check which required outputs exist.

    Returns:
        Tuple of (present, missing) lists
    """
    present = []
    missing = []
    for rel_path in required:
        rel_norm = _normalize_rel_path(rel_path)
        if not rel_norm:
            continue
        full_path = os.path.join(work_dir, rel_norm)
        if os.path.exists(full_path):
            present.append(rel_norm)
        else:
            missing.append(rel_norm)
    return present, missing


def _resolve_execute_code_mode(payload: Dict[str, Any]) -> Tuple[str, List[str], set[str]]:
    """
    Resolve execute_code behavior from payload mode.

    Returns:
        (mode, required_outputs, skip_paths_for_upload_scan)
    """
    mode = str(payload.get("mode") or "execute_code").strip().lower()
    required_payload = payload.get("required_outputs")
    required = []
    if isinstance(required_payload, list):
        raw_paths = []
        for item in required_payload:
            if isinstance(item, dict):
                path = item.get("path") or ""
            elif isinstance(item, str):
                path = item
            else:
                continue
            if path:
                raw_paths.append(path)
        required = [_normalize_rel_path(path) for path in raw_paths if path]
        required = [path for path in required if path]
        required = list(dict.fromkeys(required))
    # Accept both legacy and explicit DE mode tags.
    if mode in {"data_engineer_cleaning", "data_engineer"}:
        mode = "data_engineer_cleaning"
        # Contract-driven required outputs for DE are supplied by orchestrator.
        skip_paths = {"data/cleaned_full.csv"}
        return mode, required, skip_paths
    # For ML, treat cleaned datasets as input artifacts to avoid re-upload noise.
    skip_paths = {
        "data/cleaned_data.csv",
        "data/cleaned_full.csv",
    }
    return mode, required, skip_paths


def execute_code_mode(payload: Dict[str, Any], output_uri: str, run_id: str) -> int:
    """
    Execute user-provided Python script (same behavior as E2B sandbox).

    This mode:
    1. Downloads dataset to payload data_path (contract/orchestrator supplied)
    2. Downloads support files (manifests, column configs, etc.)
    3. Executes the ML script from the working directory
    4. Collects and uploads all generated outputs
    5. Validates required outputs exist

    Args:
        payload: Request payload with code_uri, dataset_uri, etc.
        output_uri: GCS URI prefix for outputs
        run_id: Run identifier for logging

    Returns:
        Exit code (0 for success, non-zero for failure)
    """
    work_dir = "/tmp/run"
    os.makedirs(work_dir, exist_ok=True)

    mode, required_outputs, skip_paths = _resolve_execute_code_mode(payload)
    requested_mode = str(payload.get("mode") or "execute_code").strip().lower()
    log(
        "execute_code.mode="
        + str(mode)
        + f"; requested_mode={requested_mode}; required_outputs={required_outputs}"
    )
    script_timeout_seconds = _resolve_script_timeout_seconds(payload)
    log(f"execute_code.script_timeout_seconds={script_timeout_seconds}")

    # Setup data directories (same structure as E2B)
    for subdir in ["data", "static/plots", "report", "artifacts"]:
        os.makedirs(os.path.join(work_dir, subdir), exist_ok=True)

    # Download dataset
    dataset_uri = payload.get("dataset_uri")
    if not dataset_uri:
        raise ValueError("dataset_uri is required")

    data_path = payload.get("data_path")
    if not data_path:
        raise ValueError("data_path is required for execute_code mode")
    data_full_path = os.path.join(work_dir, data_path)
    os.makedirs(os.path.dirname(data_full_path), exist_ok=True)

    log(f"Downloading dataset from {dataset_uri}")
    log(f"  -> {data_path}")
    _download_to_path(dataset_uri, data_full_path)

    # Download support files
    support_files = payload.get("support_files")
    if support_files:
        log("Downloading support files:")
        _download_support_files(support_files, work_dir)

    # Download and prepare script
    code_uri = payload.get("code_uri")
    if not code_uri:
        raise ValueError("code_uri is required for execute_code mode")

    script_path = os.path.join(work_dir, "ml_script.py")
    role_label = "Data Engineer" if mode == "data_engineer_cleaning" else "ML Engineer"
    log(f"Downloading {role_label} script from {code_uri}")
    _download_to_path(code_uri, script_path)

    dynamic_dep_install_enabled = str(os.getenv("HEAVY_RUNNER_DYNAMIC_DEP_INSTALL", "1")).strip().lower() not in {
        "0",
        "false",
        "no",
        "off",
    }
    if dynamic_dep_install_enabled:
        dep_plan = _resolve_dynamic_dependency_plan(payload, script_path)
        unknown_roots = dep_plan.get("unknown_roots") or []
        if unknown_roots:
            log(f"Dependency roots ignored (not in dynamic allowlist): {unknown_roots}")
        pip_packages = dep_plan.get("pip_packages") or []
        if pip_packages:
            _install_dynamic_dependencies([str(pkg) for pkg in pip_packages if str(pkg).strip()])

    # Execute script
    log("=" * 60)
    log(f"EXECUTING SCRIPT ({role_label}, mode={mode})")
    log("=" * 60)

    exec_start = time.perf_counter()
    exit_code, stdout, stderr = _run_script(
        script_path,
        work_dir,
        timeout_seconds=script_timeout_seconds,
    )
    exec_time = time.perf_counter() - exec_start
    stdout = _ensure_text(stdout)
    stderr = _ensure_text(stderr)

    log(f"Execution completed in {exec_time:.2f}s with exit_code={exit_code}")

    # Save execution log
    log_content = []
    log_content.append(f"=== HEAVY RUNNER EXECUTION LOG ===")
    log_content.append(f"Run ID: {run_id}")
    log_content.append(f"Exit Code: {exit_code}")
    log_content.append(f"Execution Time: {exec_time:.2f}s")
    log_content.append("")
    log_content.append("=== STDOUT ===")
    log_content.append(stdout or "(empty)")
    log_content.append("")
    log_content.append("=== STDERR ===")
    log_content.append(stderr or "(empty)")

    log_path = os.path.join("/tmp", "execution_log.txt")
    with open(log_path, "w", encoding="utf-8") as f:
        f.write("\n".join(log_content))
    _write_file_output(log_path, output_uri, "artifacts/execution_log.txt")

    # Print stdout/stderr to Cloud Run logs for debugging
    if stdout:
        log("--- Script STDOUT ---")
        for line in stdout.splitlines()[:100]:  # Limit to first 100 lines
            print(f"  {line}")
        if len(stdout.splitlines()) > 100:
            print(f"  ... ({len(stdout.splitlines()) - 100} more lines)")

    if stderr:
        log("--- Script STDERR ---")
        for line in stderr.splitlines()[:50]:  # Limit to first 50 lines
            print(f"  {line}")

    # Check for execution failure
    if exit_code != 0:
        error_msg = f"{role_label} script failed with exit_code={exit_code} (mode={mode})"
        if "Traceback" in stderr:
            # Extract last traceback
            tb_lines = stderr.split("Traceback")[-1]
            error_msg += f"\n\nTraceback:{tb_lines[:2000]}"
        raise RuntimeError(error_msg)

    # Collect and upload outputs
    log("Collecting output files...")
    collected_outputs = _collect_output_files(work_dir, skip_paths=skip_paths)
    collected_outputs.extend(
        _collect_existing_required_outputs(
            work_dir=work_dir,
            required=required_outputs,
            skip_paths=skip_paths,
        )
    )
    collected_outputs = sorted(set(collected_outputs))
    log(f"Found {len(collected_outputs)} output files")

    uploaded = []
    for rel_path in collected_outputs:
        local_path = os.path.join(work_dir, rel_path)
        if os.path.exists(local_path):
            _write_file_output(local_path, output_uri, rel_path)
            uploaded.append(rel_path)
            log(f"  Uploaded: {rel_path}")

    # Verify required outputs for this execute_code mode
    present, missing = _check_required_outputs(work_dir, required_outputs)

    if missing:
        log(f"WARNING: Missing contract-required outputs: {missing}")
        _write_json_output(
            {
                "ok": False,
                "run_id": run_id,
                "runner_protocol_version": HEAVY_RUNNER_PROTOCOL_VERSION,
                "requested_mode": requested_mode,
                "resolved_mode": mode,
                "error": "missing_required_outputs",
                "missing": missing,
                "present": present,
                "uploaded": uploaded,
                "stdout_tail": stdout[-2000:] if stdout else "",
                "stderr_tail": stderr[-2000:] if stderr else "",
            },
            output_uri,
            "error.json",
        )
        raise RuntimeError(f"Script completed but missing required outputs: {missing}")

    # Write success status
    status_payload = {
        "ok": True,
        "run_id": run_id,
        "mode": "execute_code",
        "execute_code_mode": mode,
        "requested_mode": requested_mode,
        "runner_protocol_version": HEAVY_RUNNER_PROTOCOL_VERSION,
        "exit_code": exit_code,
        "execution_time_seconds": round(exec_time, 2),
        "uploaded_outputs": uploaded,
        "required_outputs_expected": required_outputs,
        "required_outputs_present": present,
        "required_outputs_missing": missing,
    }
    _write_json_output(status_payload, output_uri, "status.json")

    log("=" * 60)
    log("EXECUTION COMPLETED SUCCESSFULLY")
    log(f"Uploaded {len(uploaded)} files")
    log("=" * 60)

    return 0


def train_mode(payload: Dict[str, Any], output_uri: str, run_id: str) -> int:
    """
    Built-in training mode (fallback when no code_uri provided).

    This mode uses the request parameters to train a model directly,
    without executing a user-provided script.
    """
    dataset_uri = payload.get("dataset_uri")
    if not dataset_uri:
        raise ValueError("dataset_uri is required")

    read_cfg = payload.get("read") or {}
    target_col = payload.get("target_col")
    if not target_col:
        raise ValueError("target_col is required")

    feature_cols = payload.get("feature_cols")
    problem_type = payload.get("problem_type", "").lower().strip()
    if problem_type not in {"classification", "regression"}:
        raise ValueError("problem_type must be 'classification' or 'regression'")

    float32 = bool(payload.get("float32"))
    safe_mode = bool(payload.get("safe_mode"))
    model_cfg = payload.get("model") or {}
    cv_cfg = payload.get("cv") or {}
    decisioning_required = payload.get("decisioning_required_names") or []
    if not isinstance(decisioning_required, list):
        decisioning_required = []

    # Read dataset
    log(f"Loading dataset from {dataset_uri}")
    df = _read_dataset(dataset_uri, read_cfg)

    if target_col not in df.columns:
        raise ValueError(f"Target column '{target_col}' not found in dataset")

    # Prepare features
    drop_cols = [target_col]
    if "__split" in df.columns:
        drop_cols.append("__split")

    if feature_cols:
        missing = [c for c in feature_cols if c not in df.columns]
        if missing:
            raise ValueError(f"feature_cols missing from dataset: {missing[:10]}")
        X = df[feature_cols]
    else:
        X = df.drop(columns=drop_cols)

    y = df[target_col]
    train_mask = y.notna()
    missing_target = int((~train_mask).sum())
    if missing_target:
        log(f"Detected {missing_target} rows with NaN target; excluding from training/CV.")

    if float32:
        X = X.astype(np.float32)
    else:
        non_numeric = [c for c in X.columns if not pd.api.types.is_numeric_dtype(X[c])]
        if non_numeric:
            raise ValueError(f"Non-numeric feature columns present: {non_numeric[:10]}")

    X_train = X[train_mask]
    y_train = y[train_mask]
    if len(y_train) == 0:
        raise ValueError("No non-null target rows available for training.")

    if problem_type == "classification" and pd.api.types.is_numeric_dtype(y_train):
        y_train = y_train.astype(int)

    # Resolve model
    model, model_type, model_params = _resolve_model(problem_type, model_cfg, safe_mode)
    cv_obj, metric_name, folds, cv_n_jobs = _resolve_cv(problem_type, cv_cfg, safe_mode)

    metrics: Dict[str, Any] = {
        "run_id": run_id,
        "problem_type": problem_type,
        "model": {"type": model_type, "params": model_params},
        "data": {
            "n_rows": int(len(df)),
            "n_cols": int(df.shape[1]),
            "n_features": int(X.shape[1]),
            "n_train_rows": int(len(y_train)),
            "n_missing_target": int(missing_target),
            "target_col": target_col,
            "feature_cols": feature_cols if feature_cols else "all_except_target",
            "float32": float32,
            "dtype_summary": {k: int(v) for k, v in df.dtypes.astype(str).value_counts().items()},
        },
        "timing_seconds": {},
        "cv": {"enabled": False},
    }

    # Cross-validation
    if cv_obj is not None and len(y_train) >= (folds or 0):
        log(f"Running cross-validation: folds={folds}, metric={metric_name}")
        cv_start = time.perf_counter()
        if problem_type == "classification":
            scores = cross_val_score(model, X_train, y_train, cv=cv_obj, scoring="accuracy", n_jobs=cv_n_jobs)
            scores_list = [float(s) for s in scores]
            metrics["cv"] = {
                "enabled": True,
                "folds": folds,
                "metric": "accuracy",
                "scores": scores_list,
                "mean": float(np.mean(scores)),
                "std": float(np.std(scores)),
            }
        else:
            scores = cross_val_score(
                model, X_train, y_train, cv=cv_obj, scoring="neg_root_mean_squared_error", n_jobs=cv_n_jobs
            )
            rmse_scores = [-float(s) for s in scores]
            metrics["cv"] = {
                "enabled": True,
                "folds": folds,
                "metric": "rmse",
                "scores": rmse_scores,
                "mean": float(np.mean(rmse_scores)),
                "std": float(np.std(rmse_scores)),
            }
        metrics["timing_seconds"]["cv"] = round(time.perf_counter() - cv_start, 6)
    elif cv_obj is not None:
        metrics["cv"] = {
            "enabled": False,
            "note": "insufficient labeled rows for requested cv folds",
            "folds_requested": folds,
            "n_train_rows": int(len(y_train)),
        }

    # Train final model
    log("Fitting final model on full dataset.")
    train_start = time.perf_counter()
    model.fit(X_train, y_train)
    metrics["timing_seconds"]["train"] = round(time.perf_counter() - train_start, 6)

    # Generate predictions
    log("Generating predictions for scored rows.")
    preds = model.predict(X)
    scored_df = df.copy()
    scored_df["prediction"] = preds

    train_metrics: Dict[str, Any] = {}
    if problem_type == "classification":
        probs = model.predict_proba(X)
        max_prob = np.max(probs, axis=1)
        scored_df["probability"] = max_prob
        scored_df["confidence_score"] = max_prob
        train_metrics["accuracy"] = float(np.mean(preds[train_mask] == y_train.values))
    else:
        scored_df["predicted_value"] = preds
        rmse = float(np.sqrt(np.mean((preds[train_mask] - y_train.values) ** 2)))
        train_metrics["rmse"] = rmse

    # Handle decisioning columns
    if decisioning_required:
        priority_score = None
        if "priority_score" in decisioning_required:
            if problem_type == "classification" and "probability" in scored_df.columns:
                priority_score = scored_df["probability"].astype(float).to_numpy()
            else:
                preds_arr = np.asarray(preds, dtype=float)
                min_val = float(np.min(preds_arr))
                max_val = float(np.max(preds_arr))
                denom = max_val - min_val
                if denom <= 0:
                    priority_score = np.zeros_like(preds_arr)
                else:
                    priority_score = (preds_arr - min_val) / denom
            scored_df["priority_score"] = priority_score
        if "priority_rank" in decisioning_required:
            if priority_score is None:
                priority_score = scored_df.get("priority_score", pd.Series(np.zeros(len(scored_df))))
            ranks = pd.Series(priority_score).rank(method="first", ascending=False).astype(int)
            scored_df["priority_rank"] = ranks

    metrics["train"] = train_metrics

    # Write outputs
    metrics_path = os.path.join("/tmp", "metrics.json")
    with open(metrics_path, "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=True)
    _write_file_output(metrics_path, output_uri, "metrics.json")

    model_path = os.path.join("/tmp", "model.joblib")
    joblib.dump(model, model_path)
    _write_file_output(model_path, output_uri, "model.joblib")

    alignment_reqs = []
    if metrics.get("cv", {}).get("enabled"):
        alignment_reqs.append({
            "name": f"cv_{metrics['cv'].get('metric')}",
            "status": "PASS",
            "value": metrics["cv"].get("mean"),
        })
    if train_metrics:
        for key, val in train_metrics.items():
            alignment_reqs.append({"name": f"train_{key}", "status": "PASS", "value": val})

    alignment_check = {
        "status": "PASS",
        "summary": "Heavy runner training completed.",
        "requirements": alignment_reqs,
        "feature_usage": {
            "used_features": feature_cols if feature_cols else "all_except_target",
            "target_columns": [target_col],
        },
    }
    alignment_path = os.path.join("/tmp", "alignment_check.json")
    with open(alignment_path, "w", encoding="utf-8") as f:
        json.dump(alignment_check, f, indent=2, ensure_ascii=True)
    _write_file_output(alignment_path, output_uri, "alignment_check.json")

    scored_path = os.path.join("/tmp", "scored_rows.csv")
    sep = read_cfg.get("sep", ",")
    decimal = read_cfg.get("decimal", ".")
    encoding = read_cfg.get("encoding", "utf-8")
    scored_df.to_csv(scored_path, index=False, sep=sep, decimal=decimal, encoding=encoding)
    _write_file_output(scored_path, output_uri, "scored_rows.csv")

    log("Training completed successfully.")
    return 0


def _read_dataset(dataset_uri: str, read_cfg: Dict[str, Any]) -> pd.DataFrame:
    """Read dataset from URI with dialect configuration."""
    local_path = dataset_uri
    if dataset_uri.startswith("gs://"):
        local_path = os.path.join("/tmp", os.path.basename(dataset_uri))
        _download_gcs_to_path(dataset_uri, local_path)
    ext = os.path.splitext(local_path)[1].lower()
    if ext in {".parquet", ".pq"}:
        try:
            return pd.read_parquet(local_path)
        except Exception as exc:
            raise ValueError(f"Failed to read parquet: {exc}") from exc
    sep = read_cfg.get("sep", ",")
    decimal = read_cfg.get("decimal", ".")
    encoding = read_cfg.get("encoding", "utf-8")
    return pd.read_csv(local_path, sep=sep, decimal=decimal, encoding=encoding)


def _resolve_model(
    problem_type: str,
    model_cfg: Dict[str, Any],
    safe_mode: bool,
) -> Tuple[Any, str, Dict[str, Any]]:
    """Resolve model configuration to sklearn estimator."""
    model_type = model_cfg.get("type")
    if not model_type:
        model_type = "random_forest_classifier" if problem_type == "classification" else "random_forest_regressor"
    params = dict(model_cfg.get("params") or {})
    if safe_mode:
        params["n_jobs"] = 1
    if model_type == "random_forest_classifier":
        return RandomForestClassifier(**params), model_type, params
    if model_type == "random_forest_regressor":
        return RandomForestRegressor(**params), model_type, params
    raise ValueError(f"Unsupported model.type: {model_type}")


def _resolve_cv(
    problem_type: str,
    cv_cfg: Dict[str, Any],
    safe_mode: bool,
) -> Tuple[Optional[Any], Optional[str], Optional[int], Optional[int]]:
    """Resolve cross-validation configuration."""
    folds = cv_cfg.get("folds")
    if folds is None:
        return None, None, None, None
    try:
        folds = int(folds)
    except Exception:
        raise ValueError("cv.folds must be an integer")
    if folds < 2:
        return None, None, None, None
    if safe_mode and folds > 3:
        folds = 3
    shuffle = bool(cv_cfg.get("shuffle", True))
    random_state = cv_cfg.get("random_state", 42)
    cv_n_jobs = cv_cfg.get("n_jobs")
    if safe_mode:
        cv_n_jobs = 1
    if problem_type == "classification":
        return StratifiedKFold(n_splits=folds, shuffle=shuffle, random_state=random_state), "accuracy", folds, cv_n_jobs
    return KFold(n_splits=folds, shuffle=shuffle, random_state=random_state), "rmse", folds, cv_n_jobs


def main() -> int:
    """Main entry point for Heavy Runner."""
    log("=" * 60)
    log("HEAVY RUNNER STARTED")
    log("=" * 60)

    input_uri = os.getenv("INPUT_URI", "").strip()
    output_uri = os.getenv("OUTPUT_URI", "").strip()

    if not input_uri:
        log("ERROR: Missing INPUT_URI env var.")
        return 1

    log(f"INPUT_URI: {input_uri}")
    log(f"OUTPUT_URI: {output_uri}")

    try:
        payload = _load_input_json(input_uri)
    except Exception as exc:
        log(f"ERROR: Failed to load input JSON: {exc}")
        return 1

    output_uri = _normalize_output_uri(output_uri or payload.get("output_uri", ""))
    if not output_uri:
        log("ERROR: Missing OUTPUT_URI env var and no output_uri in input JSON.")
        return 1

    run_id = payload.get("run_id") or "unknown"
    log(f"Run ID: {run_id}")

    # Handle ping request
    if payload.get("ping") == "ok":
        log("Received ping request. Writing status.json.")
        _write_json_output({"ok": True, "run_id": run_id}, output_uri, "status.json")
        return 0

    try:
        # Determine execution mode
        code_uri = payload.get("code_uri")

        if code_uri:
            log("Mode: EXECUTE_CODE (running user script)")
            return execute_code_mode(payload, output_uri, run_id)
        else:
            log("Mode: TRAIN (built-in training)")
            return train_mode(payload, output_uri, run_id)

    except Exception as exc:
        stack = traceback.format_exc()
        log(f"ERROR: {exc}")
        log(stack)
        try:
            _write_json_output(
                {
                    "ok": False,
                    "run_id": run_id,
                    "error": str(exc),
                    "stacktrace": stack,
                },
                output_uri,
                "error.json",
            )
        except Exception as write_err:
            log(f"Failed to write error.json: {write_err}")
        return 2


if __name__ == "__main__":
    sys.exit(main())
