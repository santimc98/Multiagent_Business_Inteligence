import json
import os

from src.utils.governance import build_run_summary


def test_run_summary_outcome_with_limitations(tmp_path, monkeypatch):
    monkeypatch.chdir(tmp_path)
    os.makedirs("data", exist_ok=True)
    with open("data/metrics.json", "w", encoding="utf-8") as f:
        json.dump({"auc": 0.51, "baseline_auc": 0.5}, f)
    with open("data/output_contract_report.json", "w", encoding="utf-8") as f:
        json.dump({"missing": []}, f)
    with open("data/execution_contract.json", "w", encoding="utf-8") as f:
        json.dump({"counterfactual_policy": "observational_only"}, f)
    summary = build_run_summary({"review_verdict": "APPROVED"})
    assert summary.get("run_outcome") == "GO_WITH_LIMITATIONS"
    assert summary.get("metric_ceiling_detected") is True


def test_run_summary_integrity_critical_forces_no_go(tmp_path, monkeypatch):
    monkeypatch.chdir(tmp_path)
    os.makedirs("data", exist_ok=True)
    with open("data/metrics.json", "w", encoding="utf-8") as f:
        json.dump({"auc": 0.6}, f)
    with open("data/output_contract_report.json", "w", encoding="utf-8") as f:
        json.dump({"missing": []}, f)
    with open("data/integrity_audit_report.json", "w", encoding="utf-8") as f:
        json.dump(
            {"issues": [{"type": "MISSING_COLUMN", "severity": "critical"}]},
            f,
        )

    summary = build_run_summary({"review_verdict": "APPROVED"})
    assert summary.get("run_outcome") == "NO_GO"
    assert "integrity_critical" in summary.get("failed_gates", [])
    assert summary.get("integrity_critical_count") == 1


def test_run_summary_integrity_warning_does_not_force_no_go(tmp_path, monkeypatch):
    monkeypatch.chdir(tmp_path)
    os.makedirs("data", exist_ok=True)
    with open("data/metrics.json", "w", encoding="utf-8") as f:
        json.dump({"auc": 0.6}, f)
    with open("data/output_contract_report.json", "w", encoding="utf-8") as f:
        json.dump({"missing": []}, f)
    with open("data/integrity_audit_report.json", "w", encoding="utf-8") as f:
        json.dump(
            {"issues": [{"type": "OPTIONAL_COLUMN_MISSING", "severity": "warning"}]},
            f,
        )

    summary = build_run_summary({"review_verdict": "APPROVED"})
    assert summary.get("integrity_critical_count") == 0
    assert "integrity_critical" not in summary.get("failed_gates", [])
    assert summary.get("run_outcome") in {"GO", "GO_WITH_LIMITATIONS"}


def test_run_summary_reports_all_contract_views(tmp_path, monkeypatch):
    monkeypatch.chdir(tmp_path)
    views_dir = os.path.join("data", "contracts", "views")
    os.makedirs(views_dir, exist_ok=True)
    for name in [
        "de_view",
        "ml_view",
        "cleaning_view",
        "qa_view",
        "reviewer_view",
        "translator_view",
        "results_advisor_view",
    ]:
        with open(os.path.join(views_dir, f"{name}.json"), "w", encoding="utf-8") as f:
            json.dump({"role": name}, f)

    summary = build_run_summary({"review_verdict": "APPROVED"})
    present = set((summary.get("contract_views") or {}).get("present") or [])
    assert "cleaning_view" in present
    assert "qa_view" in present


def test_run_summary_drops_broad_qa_gate_when_qa_packet_has_no_findings(tmp_path, monkeypatch):
    monkeypatch.chdir(tmp_path)
    os.makedirs("data", exist_ok=True)
    with open("data/output_contract_report.json", "w", encoding="utf-8") as f:
        json.dump({"missing": []}, f)

    state = {
        "review_verdict": "APPROVE_WITH_WARNINGS",
        "last_gate_context": {"status": "APPROVE_WITH_WARNINGS", "failed_gates": ["qa_gates"]},
        "qa_last_result": {"status": "APPROVE_WITH_WARNINGS", "failed_gates": [], "hard_failures": []},
    }
    summary = build_run_summary(state)
    assert "qa_gates" not in (summary.get("failed_gates") or [])


def test_run_summary_ignores_stale_pipeline_aborted_reason_when_metrics_exist(tmp_path, monkeypatch):
    monkeypatch.chdir(tmp_path)
    os.makedirs("data", exist_ok=True)
    os.makedirs("reports", exist_ok=True)
    with open("data/output_contract_report.json", "w", encoding="utf-8") as f:
        json.dump({"missing": []}, f)
    with open("reports/evaluation_metrics.json", "w", encoding="utf-8") as f:
        json.dump(
            {
                "model_performance": {
                    "primary_metric": "RMSLE",
                    "primary_metric_value": 0.4249,
                    "cv_rmsle_mean": 0.4249,
                }
            },
            f,
        )
    with open("data/data_adequacy_report.json", "w", encoding="utf-8") as f:
        json.dump(
            {
                "status": "insufficient_signal",
                "reasons": ["pipeline_aborted_before_metrics"],
                "recommendations": [],
                "quality_gates_alignment": {"status": "partial", "mapped_gates": {}, "unmapped_gates": {}},
            },
            f,
        )

    summary = build_run_summary({"review_verdict": "APPROVED"})
    assert summary.get("metrics", {}).get("metric_pool_size", 0) > 0
    assert summary.get("metric_ceiling_detected") is False
